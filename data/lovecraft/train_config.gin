Dataset.batch_size=128
Dataset.token_count=256
Dataset.pad_id=0
Dataset.vocab_size=8192

make_optimizer.learning_rate=1e-4
make_optimizer.weight_decay=1e-1

TransformerDecoder.dimension=64
TransformerDecoder.num_heads=4
TransformerDecoder.dimension_inner=128
TransformerDecoder.num_blocks=4
TransformerDecoder.num_embeddings=8192

create_train_state.model=@TransformerDecoder()
train.num_steps=10000
train.vocab_size=8192
train.random_seed=42
train.val_freq=200